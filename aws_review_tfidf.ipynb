{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'overall', 'verified', 'reviewTime', 'reviewerID', 'asin',\n",
      "       'style', 'reviewerName', 'reviewText', 'summary', 'unixReviewTime',\n",
      "       'vote', 'image'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size = 1000\n",
    "chunks = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\aws_review_sofware_dataset.csv\", sep=',', chunksize=chunk_size)\n",
    "\n",
    "# Get the first chunk and access its columns\n",
    "df = next(chunks)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"words\"]=\"default value\"\n",
    "df[\"sentences\"]=\"default value\"\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    df.at[i,\"words\"]= list(\"\")\n",
    "    df.at[i,\"sentences\"] = list(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('C:\\\\Users\\\\HP\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizers\\\\punkt')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.find('tokenizers/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\HP/nltk_data', 'c:\\\\Program Files\\\\Python312\\\\nltk_data', 'c:\\\\Program Files\\\\Python312\\\\share\\\\nltk_data', 'c:\\\\Program Files\\\\Python312\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\HP\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('c:\\\\Program Files\\\\Python312\\\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The materials arrived early and were in excellent condition.', \"However for the money spent they really should've come with a binder and not just loose leaf.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "# Example text\n",
    "text = str(df.loc[0, \"reviewText\"])\n",
    "\n",
    "# Split sentences using a regex pattern\n",
    "sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pywsd in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (1.2.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pywsd) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pywsd) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pywsd) (2.2.2)\n",
      "Requirement already satisfied: wn==0.0.23 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pywsd) (0.0.23)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pywsd) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from nltk->pywsd) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from nltk->pywsd) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from nltk->pywsd) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from nltk->pywsd) (4.67.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pandas->pywsd) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pandas->pywsd) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pandas->pywsd) (2024.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk->pywsd) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pywsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.utils import lemmatize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(df.shape[0]):\n",
    "    df.at[k,\"words\"]=list(\"\")\n",
    "    for j in range(len(df.loc[k,\"sentences\"])):\n",
    "        df.at[k,\"words\"].extend(lemmatize_sentence(df.loc[k,\"sentences\"][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"words_sentences\"] = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "for k in range(df.shape[0]):\n",
    "    words = df.loc[k, \"words\"]\n",
    "    if words:  # Check if the list is not empty\n",
    "        df.loc[k, \"words_sentences\"] = functools.reduce(lambda a, b: str(a) + \" \" + str(b), words)\n",
    "    else:\n",
    "        df.loc[k, \"words_sentences\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11364\\4244777884.py:5: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\aws_review_sofware_dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF matrix: (459350, 1000)\n",
      "Vocabulary size: 1000\n",
      "Vocabulary: {'the': 843, 'arrived': 88, 'and': 68, 'were': 955, 'in': 410, 'excellent': 286, 'however': 397, 'for': 324, 'money': 531, 'spent': 796, 'they': 849, 'really': 697, 'should': 766, 've': 925, 'come': 169, 'with': 971, 'not': 566, 'just': 440, 'am': 63, 'this': 855, 'book': 121, 'that': 842, 'make': 502, 'you': 997, 'review': 720, 'your': 998, 'what': 956, 'to': 863, 'do': 235, 'when': 957, 'it': 434, 'me': 516, 'of': 572, 'my': 544, 'if': 401, 'are': 85, 'taking': 829, 'class': 160, 'don': 241, 'waste': 944, 'on': 582, 'so': 779, 'called': 139, '00': 0, 'even': 278, 'had': 366, 'provide': 672, 'own': 607, 'out': 603, 'can': 141, 'buy': 135, 'access': 34, 'at': 93, 'is': 430, 'complete': 174, 'was': 942, 'missing': 527, 'pages': 610, 'important': 406, 'couldn': 193, 'answer': 71, 'some': 782, 'test': 837, 'questions': 682, 'because': 109, 'have': 374, 'never': 556, 'before': 112, 'used': 915, 'say': 734, 'an': 67, 'amazing': 64, 'tool': 869, 'quickly': 686, 'simply': 772, 'learning': 459, 'go': 355, 'once': 583, 'start': 800, 'using': 921, 'will': 966, 'best': 115, 'way': 946, 'learn': 457, 'business': 132, 'student': 811, 'did': 225, 'use': 914, 'accounting': 36, 'difference': 227, 'from': 335, 'whole': 963, 'thanks': 841, 'every': 280, 'definitely': 216, 'check': 155, 'good': 359, 'read': 693, 'quite': 687, 'up': 904, 'date': 210, 'takes': 828, 'bit': 119, 'live': 478, 'internet': 425, 'got': 361, 'amazon': 65, 'ended': 268, 'lot': 489, 'great': 364, 'has': 372, 'been': 111, 'extremely': 293, 'useful': 916, 'very': 928, 'happy': 369, 'purchase': 675, 'fast': 299, 'but': 133, 'be': 108, 'again': 52, 'site': 775, 'school': 739, 'books': 122, 'saved': 733, 'over': 605, 'maybe': 514, 'no': 561, 'background': 101, 'want': 939, 'cannot': 142, 'get': 347, 'anything': 76, 'text': 838, 'tried': 881, 'hard': 370, 'through': 860, 'writing': 989, 'something': 784, 'after': 51, 'seems': 748, 'same': 730, 'thing': 850, 'message': 521, 'almost': 57, 'like': 469, 'reading': 694, 'paper': 612, 'their': 844, 'long': 482, 'words': 977, 'order': 595, 'sound': 788, 'off': 573, 'paid': 611, 'most': 535, 'useless': 917, 'research': 714, 'process': 662, 'other': 599, 'far': 298, 'help': 378, 'only': 587, 'reason': 698, 'giving': 353, 'two': 893, 'stars': 799, 'one': 584, 'star': 798, 'first': 318, 'them': 845, 'actually': 42, 'made': 498, 'job': 439, 'helpful': 380, 'information': 417, 'going': 357, 'second': 742, 'gave': 346, 'lots': 490, 'info': 416, 'easily': 258, 'unfortunately': 899, 'pretty': 653, 'much': 540, 'getting': 349, 'old': 580, 'course': 195, 'able': 30, 'understand': 898, 'stay': 804, 'while': 961, 'worth': 985, 'price': 655, 'also': 60, 'little': 477, 'times': 862, 'as': 90, 'would': 986, 'cost': 191, 'someone': 783, 'who': 962, 'need': 550, 'by': 137, 'number': 570, 'during': 252, 'case': 146, 'videos': 931, 'more': 534, 'non': 562, 'those': 856, 'available': 96, 'find': 315, 'less': 462, 'does': 238, 'how': 396, 'set': 758, 'online': 586, 'store': 810, 'write': 988, 'etc': 277, 'looking': 486, 'view': 932, 'big': 118, 'picture': 633, 'recommend': 701, 'than': 839, 'required': 712, 'love': 491, 'fact': 294, 'graphics': 363, 'lost': 488, 'any': 74, 'list': 476, 'tools': 870, 'which': 960, 'makes': 503, 'must': 543, 'doing': 240, 'charge': 152, 'purchased': 676, 'thank': 840, 'faster': 300, 'then': 846, 'told': 866, 'well': 953, 'written': 990, 'we': 947, 'didn': 226, 'look': 484, 'web': 948, 'color': 167, 'expected': 289, 'least': 460, 'certain': 148, 'example': 284, 'brand': 128, 'uses': 920, 'since': 773, 'bought': 126, 'these': 848, 'items': 436, 'idea': 399, 'difficult': 229, 'trust': 884, 'simple': 771, 'could': 192, 'correct': 189, 'errors': 275, 'along': 58, 'know': 449, 'provides': 674, 'many': 508, 'about': 31, 'basic': 106, 'enjoy': 270, 'years': 994, 'high': 384, 'level': 465, 'version': 926, 'doesn': 239, 'into': 426, 'enough': 271, 'hand': 368, 'design': 220, 'necessary': 549, 'goes': 356, 'create': 199, 'within': 972, 'expect': 288, 'entire': 273, 'system': 824, 'under': 897, 'note': 567, 'there': 847, 'actual': 41, 'user': 918, 'interface': 424, 'overall': 606, 'experience': 291, 'different': 228, 'isn': 431, 'here': 383, 'possible': 647, 'absolutely': 33, 'speak': 791, 'said': 729, 'above': 32, 're': 692, 'or': 594, 'wants': 941, 'done': 242, 'pc': 620, 'mac': 496, 'needed': 551, 'video': 930, 'each': 254, 'stuff': 813, 'includes': 413, 'such': 815, 'website': 950, 'creating': 201, 'new': 557, 'page': 609, 'images': 403, 'media': 518, 'forms': 328, 'content': 183, 'section': 743, 'project': 669, 'print': 656, 'll': 479, 'she': 763, 'all': 54, 'though': 857, 'her': 382, 'bad': 103, 'people': 622, 'top': 871, 'previous': 654, 'button': 134, 'next': 559, 'last': 454, 'item': 435, 'choice': 158, 'changed': 150, 'take': 827, 'instead': 422, 'end': 267, 'mouse': 536, 'area': 86, 'current': 203, 'left': 461, 'navigate': 547, 'work': 978, 'mode': 530, 'let': 464, 'copy': 187, 'included': 412, 'move': 537, 'window': 968, 'around': 87, 'may': 513, 'kind': 447, 'setup': 761, 'standard': 797, 'player': 639, 'screen': 740, 'either': 264, 'value': 923, 'hours': 394, 'training': 875, 'package': 608, 'small': 778, 'premier': 651, 'product': 664, 'offers': 576, 'several': 762, 'features': 303, 'improved': 408, 'applications': 83, 'offer': 574, 'control': 185, 'turn': 890, 'see': 745, 'box': 127, 'directly': 231, 'dvd': 253, 'versions': 927, 'keep': 442, 'software': 780, 'already': 59, 'perfect': 624, 'place': 636, 'else': 265, 'adobe': 48, 'files': 310, 'runs': 727, 'bottom': 125, 'stop': 808, 'time': 861, 'being': 113, 'try': 885, 'few': 306, 'form': 326, 'crash': 196, 'finally': 313, 'code': 166, 'setting': 759, 'our': 602, 'quick': 683, 'add': 43, 'shows': 768, 'format': 327, 'main': 500, 'five': 319, 'play': 638, 'save': 732, 'why': 964, 'three': 859, 'typing': 895, 'ordered': 596, 'options': 593, 'where': 958, 'file': 309, 'link': 473, 'edit': 260, 'created': 200, 'right': 722, 'image': 402, 'choose': 159, 'photoshop': 632, '10': 1, 'document': 236, 'single': 774, 'word': 976, 'microsoft': 523, 'excel': 285, 'email': 266, 'address': 47, '11': 3, 'fine': 316, 'four': 331, '12': 4, 'easy': 259, 'data': 208, 'working': 980, 'gets': 348, 'back': 100, 'library': 466, 'additional': 46, 'based': 105, 'application': 82, '15': 5, 'search': 741, 'fix': 320, 'issues': 433, 'allow': 55, 'users': 919, 'size': 776, 'server': 755, 'download': 244, 'editing': 261, 'allows': 56, 'technical': 834, 'anyone': 75, 'now': 569, 'exactly': 283, 'program': 667, 'thought': 858, 'its': 437, 'totally': 873, 'away': 98, 'us': 912, 'specific': 792, 'step': 805, 'haven': 375, 'sure': 819, 'negative': 553, 'started': 801, 'recently': 700, 'trying': 886, 'spending': 795, 'upgrade': 908, 'always': 62, 'upgrades': 910, 'advanced': 49, 'boot': 123, 'fairly': 296, 'myself': 545, 'later': 455, 'lessons': 463, 'short': 765, 'liked': 470, 'minutes': 526, 'won': 974, 'low': 494, 'designed': 221, 'highly': 385, 'intuitive': 428, 'seen': 749, 'sometimes': 785, 'close': 164, 'things': 851, 'wonderful': 975, 'decided': 215, 'products': 665, 'switch': 820, 'still': 806, 'found': 330, 'think': 852, 'both': 124, 'part': 614, 'changes': 151, 'curve': 204, 'comes': 170, 'addition': 45, 'itself': 438, 'might': 524, 'large': 453, 'question': 681, 'better': 116, 'everything': 282, 'seem': 746, 'helped': 379, 'give': 350, 'ease': 256, 'gives': 352, 'too': 867, 'slow': 777, 'instructions': 423, 'down': 243, 'hour': 393, 'problem': 660, 'looks': 487, 'show': 767, 'update': 905, 'wanted': 940, 'needs': 552, 'photo': 630, 'maps': 510, 'yes': 995, 'wish': 970, 'total': 872, 'worked': 979, 'programs': 668, 'perfectly': 625, 'point': 644, 'learned': 458, 'making': 504, 'personal': 628, 'practice': 650, 'functionality': 340, 'clear': 162, 'audio': 94, 'nice': 560, 'cut': 207, 'app': 78, 'function': 339, 'without': 973, 'expensive': 290, 'tech': 833, 'support': 817, 'wasn': 943, 'voice': 936, 'although': 61, 'took': 868, 'feel': 305, 'until': 903, 'seemed': 747, 'folder': 322, 'opinion': 591, 'ask': 91, 'deal': 214, 'disc': 233, 'installing': 421, 'computer': 177, 'running': 726, 'antivirus': 73, 'gone': 358, 'yet': 996, 'malware': 505, 'select': 750, 'name': 546, 'together': 865, 'helps': 381, 'having': 376, 'follow': 323, 'major': 501, 'came': 140, 'trial': 880, 'connection': 179, 'speed': 793, 'onto': 588, 'immediately': 404, 'kept': 444, 'recommended': 702, 'real': 696, 'full': 337, 'type': 894, 'buying': 136, 'hook': 390, 'linked': 474, 'normal': 564, 'ie': 400, 'completely': 175, 'year': 993, 'probably': 659, '30': 22, 'mind': 525, 'earlier': 255, 'another': 70, 'spend': 794, 'nothing': 568, 'ago': 53, 'os': 598, 'professional': 666, 'drive': 249, 'change': 149, 'open': 589, 'windows': 969, 'functions': 341, 'documents': 237, 'none': 563, 'says': 736, 'via': 929, 'menu': 520, 'wasted': 945, 'mistake': 528, 'rather': 690, 'saying': 735, 'wouldn': 987, 'added': 44, 'photos': 631, 'apparently': 79, 'machine': 497, 'free': 332, 'paying': 618, 'month': 532, 'com': 168, 'half': 367, 'quality': 680, 'otherwise': 601, 'pop': 646, 'requires': 713, 'various': 924, 'home': 389, 'click': 163, 'english': 269, 'compared': 172, 'nearly': 548, 'ready': 695, 'enter': 272, 'credit': 202, 'wait': 937, 'months': 533, 'future': 342, 'line': 472, 'key': 445, 'favorite': 301, 'desktop': 222, 'error': 274, 'issue': 432, 'option': 592, 'upgrading': 911, 'usually': 922, 'ability': 29, 'tell': 835, 'limited': 471, 'family': 297, 'including': 414, 'become': 110, 'matter': 512, 'ever': 279, 'past': 616, 'older': 581, 'easier': 257, 'laptop': 452, 'person': 627, 'especially': 276, 'true': 883, 'digital': 230, 'forward': 329, 'went': 954, 'upgraded': 909, 'wrong': 991, 'feature': 302, 'between': 117, 'couple': 194, 'known': 450, 'works': 981, 'believe': 114, 'offered': 575, '100': 2, '50': 25, 'given': 351, 'disk': 234, 'playing': 640, 'he': 377, 'world': 982, 'piece': 635, 'his': 387, 'fixed': 321, 'pleased': 642, 'run': 725, 'phone': 629, 'pay': 617, 'complicated': 176, 'starting': 802, 'remove': 708, 'settings': 760, 'automatically': 95, 'numbers': 571, 'similar': 770, 'rating': 691, 'put': 678, 'install': 418, 'installation': 419, 'annoying': 69, 'ones': 585, 'installed': 420, 'pictures': 634, 'whether': 959, 'block': 120, 'provided': 673, 'often': 578, 'figured': 308, 'figure': 307, 'worse': 983, 'soon': 787, 'import': 405, 'twice': 892, 'impressed': 407, 'disappointed': 232, 'serious': 754, 'remember': 707, 'effects': 263, 'awesome': 99, 'huge': 398, 'basically': 107, 'third': 854, 'supposed': 818, 'trouble': 882, 'plus': 643, 'side': 769, 'track': 874, 'investment': 429, 'properly': 670, 'switched': 821, 'friendly': 333, 'scan': 737, 'include': 411, 'convert': 186, 'manual': 506, 'frustrating': 336, 'language': 451, 'backup': 102, 'means': 517, 'original': 597, 'operating': 590, 'appears': 80, 'built': 131, 'customer': 205, 'yourself': 999, 'movie': 538, 'powerful': 649, 'latest': 456, 'suite': 816, 'ms': 539, 'cd': 147, 'unable': 896, 'plan': 637, 'market': 511, '20': 6, 'multiple': 541, 'keeps': 443, 'linux': 475, 'device': 223, 'luck': 495, 'care': 145, 'friends': 334, 'extra': 292, 'newer': 558, 'problems': 661, 'office': 577, 'company': 171, 'glad': 354, 'pro': 658, 'week': 951, 'cards': 144, '2008': 13, 'others': 600, 'games': 344, 'longer': 483, 'rosetta': 723, 'stone': 807, 'game': 343, 'fun': 338, 'downloading': 246, 'daughter': 211, 'satisfied': 731, 'weeks': 952, 'tablet': 826, 'bugs': 130, 'horrible': 392, 'worst': 984, 'memory': 519, 'continue': 184, 'report': 710, 'call': 138, 'guess': 365, 'return': 718, 'loves': 493, 'day': 212, 'activation': 40, 'mail': 499, 'service': 756, 'received': 699, 'contact': 181, 'send': 752, 'reviews': 721, 'hold': 388, 'hate': 373, 'contacted': 182, 'spanish': 790, 'loved': 492, 'except': 287, 'today': 864, 'everyone': 281, 'days': 213, 'forced': 325, '99': 28, 'failed': 295, 'response': 715, 'cheaper': 154, 'amount': 66, 'due': 251, 'children': 157, 'please': 641, 'life': 468, 'per': 623, '2011': 16, 'thinking': 853, 'hope': 391, 'downloaded': 245, 'prior': 657, 'son': 786, 'accurate': 38, 'improvement': 409, 'kids': 446, 'edition': 262, 'devices': 224, 'purchasing': 677, 'downloads': 247, 'win': 967, 'xp': 992, 'dell': 218, '2003': 8, 'apps': 84, 'systems': 825, 'power': 648, 'clean': 161, 'card': 143, 'google': 360, 'anyway': 77, 'unless': 902, '95': 27, 'poor': 645, 'ok': 579, 'child': 156, 'him': 386, 'updates': 907, 'computers': 178, 'account': 35, 'compatible': 173, 'consider': 180, 'music': 542, '2013': 18, 'license': 467, 'outlook': 604, '2010': 15, 'subscription': 814, 'load': 480, 'final': 312, 'release': 706, 'premium': 652, '2007': 12, 'space': 789, 'kindle': 448, 'transfer': 878, 'usb': 913, 'sync': 823, 'delete': 217, 'register': 704, 'performance': 626, 'vista': 935, 'ram': 688, 'processor': 663, 'pdf': 621, '2000': 7, 'manually': 507, '2016': 21, 'chat': 153, 'cloud': 165, 'accounts': 37, 'results': 717, 'quickbooks': 684, 'reinstall': 705, 'removed': 709, 'network': 555, 'security': 744, 'turned': 891, 'asked': 92, 'refund': 703, 'waiting': 938, 'map': 509, 'solution': 781, 'updated': 906, 'crashes': 198, 'terrible': 836, 'protection': 671, '360': 23, 'loaded': 481, 'advertised': 50, 'sent': 753, 'virus': 933, 'customers': 206, 'password': 615, 'uninstalled': 901, 'uninstall': 900, 'anti': 72, 'mobile': 529, 'apple': 81, 'transaction': 876, 'hardware': 371, 'wife': 965, 'firewall': 317, '64': 26, 'seller': 751, 'tax': 830, 'bank': 104, 'drivers': 250, 'norton': 565, 'house': 395, 'income': 415, 'ran': 689, 'looked': 485, 'services': 757, 'correctly': 190, 'database': 209, '2015': 20, '2014': 19, 'stopped': 809, 'financial': 314, 'dragon': 248, 'state': 803, '2012': 17, 'taxes': 832, 'art': 89, 'restore': 716, 'safe': 728, 'micro': 522, 'trend': 879, 'schedule': 738, 'parallels': 613, '2005': 10, 'budget': 129, 'transactions': 877, 'quicken': 685, 'roxio': 724, '3d': 24, 'crashed': 197, 'intuit': 427, 'turbotax': 889, '2009': 14, '2006': 11, '2004': 9, 'shop': 764, 'returns': 719, 'reports': 711, 'gps': 362, 'garmin': 345, 'studio': 812, 'corel': 188, 'viruses': 934, 'turbo': 888, 'mcafee': 515, 'deluxe': 219, 'symantec': 822, 'filing': 311, 'federal': 304, 'payroll': 619, 'qb': 679, 'nero': 554, 'avg': 97, 'kaspersky': 441, 'tt': 887, 'taxcut': 831, 'acronis': 39, 'webroot': 949}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\aws_review_sofware_dataset.csv\")\n",
    "\n",
    "\n",
    "column_name = \"reviewText\" \n",
    "df1 = df1[[column_name]] \n",
    "df1.columns = [\"words_sentences\"] \n",
    "\n",
    "\n",
    "df1 = df1[df1.words_sentences.notnull() & (df1.words_sentences != \"\")]\n",
    "df1.words_sentences = df1.words_sentences.str.lower().str.strip()\n",
    "\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_features=no_features, stop_words=None)\n",
    "\n",
    "\n",
    "try:\n",
    "    tf = tf_vectorizer.fit_transform(df1.words_sentences)\n",
    "    print(\"Shape of TF matrix:\", tf.shape)\n",
    "    print(\"Vocabulary size:\", len(tf_vectorizer.vocabulary_))\n",
    "    print(\"Vocabulary:\", tf_vectorizer.vocabulary_)\n",
    "except ValueError as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11364\\1202360519.py:7: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\aws_review_sofware_dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_x (feature matrix): (1000, 100)\n",
      "Shape of df_y_enc (target variable): (1000,)\n",
      "Random Forest Accuracy: 98.50%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\aws_review_sofware_dataset.csv\")  \n",
    "\n",
    "\n",
    "text_column = \"reviewText\"  \n",
    "target_column = \"verified\"  \n",
    "df = df[[text_column, target_column]]  \n",
    "df = df[df[text_column].notnull() & df[target_column].notnull()] \n",
    "df[text_column] = df[text_column].str.lower().str.strip()  \n",
    "\n",
    "# Step 3: Reduce dataset size\n",
    "df = df.sample(n=1000, random_state=42)  # Use only 1000 samples for faster execution\n",
    "\n",
    "# Step 4: Extract target variable\n",
    "df_y = df[target_column]\n",
    "\n",
    "# Step 5: Feature extraction\n",
    "tf_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')  # Reduce features to 100\n",
    "df_x = tf_vectorizer.fit_transform(df[text_column])  # Sparse matrix for features\n",
    "\n",
    "# Step 6: Encode the target variable\n",
    "le = LabelEncoder()\n",
    "df_y_enc = le.fit_transform(df_y)\n",
    "# Step 7: Validate shapes\n",
    "print(\"Shape of df_x (feature matrix):\", df_x.shape)\n",
    "print(\"Shape of df_y_enc (target variable):\", df_y_enc.shape)\n",
    "\n",
    "# Step 8: Train a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)  # Fewer trees for faster training\n",
    "rf.fit(df_x, df_y_enc)\n",
    "\n",
    "# Step 9: Evaluate the model\n",
    "accuracy = rf.score(df_x, df_y_enc)\n",
    "print(f\"Random Forest Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11364\\4089199371.py:7: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\aws_review_sofware_dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 69.10%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\aws_review_sofware_dataset.csv\") \n",
    "\n",
    "# Step 2: Clean the data\n",
    "text_column = \"reviewText\"  \n",
    "target_column = \"verified\" \n",
    "df = df[[text_column, target_column]]  # Select relevant columns\n",
    "df = df[df[text_column].notnull() & df[target_column].notnull()]  # Drop rows with missing values\n",
    "df[text_column] = df[text_column].str.lower().str.strip()  # Clean text column\n",
    "\n",
    "# Step 3: Reduce dataset size\n",
    "df = df.sample(n=1000, random_state=42)  # Use only 1000 samples for faster execution\n",
    "\n",
    "# Step 4: Extract target variable\n",
    "df_y = df[target_column]\n",
    "\n",
    "# Step 5: Feature extraction\n",
    "tf_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')  # Reduce features to 100\n",
    "df_x = tf_vectorizer.fit_transform(df[text_column])  # Sparse matrix for features\n",
    "# Step 6: Encode the target variable\n",
    "le = LabelEncoder()\n",
    "df_y_enc = le.fit_transform(df_y)  # Encoded target variable\n",
    "\n",
    "# Step 7: Train Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(df_x, df_y_enc)\n",
    "\n",
    "# Step 8: Evaluate Naive Bayes model\n",
    "accuracy_nb = nb.score(df_x, df_y_enc)\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC=GradientBoostingClassifier(n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_c=GBC.fit(df_x,df_y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbc_score: 98.50%\n"
     ]
    }
   ],
   "source": [
    "gbc_score=GBC.score(df_x,df_y_enc)\n",
    "print(f\"gbc_score: {gbc_score* 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
